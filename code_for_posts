#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Feb 12 13:12:45 2022

@author: avanthikasenthil
"""

def sent_to_words(sentences):
    for sentence in sentences:
        #deacc = True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]       



#Step 1 is loading the data, sp the ide can read the csv
import pandas as pd #so the ide can read the csv file
import os #so we can find the csv file

os.chdir('/Users/avanthikasenthil/Downloads/')
# Read data into papers
papers = pd.read_csv("DailyStrength Posts - dailystrength_array.csv")


import re

#removes the punction by creating a new column
#papers['Headers'] - returs the heading colum
#creating a new column that is the processed version of header
papers['Header_processed'] = \
    papers['Header'].map(lambda x: re.sub('[,\.!?]', '', x))
    #Takes the 'Header' column and takes each cell , finds any punctuation that we have entered, replaces it with nothing
    #lambda is an unamed function, the parameter is the header cell
    #x is like the return statement, after being processed, its being stored in x and returned
    
#conver it to lowercase
papers['Header_processed'] = \
    papers['Header_processed'].map(lambda x: x.lower())
    

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')
stop_words.extend(['feel', 'like', 'get', 'know', 'go', 'even', 'really', 'want', 'know', 'time', 'would', 'going', 'also', 'quot', 'much', 'think', 'take', 'things', 'one', 'could', 'need', 'hi', 'mom', 'child', 'please', 'baby', 'new', 'mommy','way', 'ever', 'sure', 'getting', 'hello', 'daughter', 'plz', 'kids', 'anyone', 'dont', 'another' , 'end', 'little','better', 'everyone', 'someone', 'got', 'old', 'away', 'read', 'tell'])

data = papers.Header_processed.values.tolist()
data_words = list(sent_to_words(data))

data_words = remove_stopwords(data_words)

print(data_words[:1][0][:30])


import gensim.corpora as corpora

#Create Dictionary
id2word = corpora.Dictionary(data_words)

#Create Corpus
texts = data_words

#Term Document Frequenct
corpus = [id2word.doc2bow(text) for text in texts]

print(corpus[:1][0][:30])

from pprint import pprint

#number of topics
num_topics = 4

#Build LDA model
lda_model = gensim.models.LdaModel(corpus = corpus, num_topics = num_topics, id2word = id2word)

pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
#Multicore - using multiple CPUs to run the method
#Splitting the corpus among multiple CPUS to run a set of words
